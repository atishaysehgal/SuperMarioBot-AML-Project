{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arpita Shah\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from __future__ import division\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Lambda, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras.backend as K\n",
    "from collections import deque\n",
    "import h5py\n",
    "import os\n",
    "import gym\n",
    "import cv2\n",
    "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.95\n",
    "        self.learning_rate = 0.5\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "    \n",
    "    # Loss function\n",
    "    def _huber_loss(self, target, prediction):\n",
    "        error = prediction - target\n",
    "        return K.mean(K.sqrt(1+K.square(error)) - 1, axis = -1)\n",
    "    \n",
    "    # Model architecture\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        #model.add(Conv2D(filters = 32, input_shape=self.state_size, kernel_size = [8,8], strides = [2,2], activation = 'relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(96, activation='tanh'))\n",
    "        model.add(Dropout(0.1))\n",
    "        #model.add(Flatten())\n",
    "        model.add(Dense(72, activation='softmax'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(48, activation='linear'))\n",
    "        model.add(Dense(20, activation='relu'))\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                     optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                # a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "                #target[0][action] = reward + self.gamma * t[np.argmax(t)]\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/10, score: 0, e: 1.0, r: 0\n",
      "episode: 0/10, score: 1, e: 1.0, r: 1\n",
      "episode: 0/10, score: 2, e: 1.0, r: 2\n",
      "episode: 0/10, score: 3, e: 1.0, r: 3\n",
      "episode: 0/10, score: 4, e: 1.0, r: 2\n",
      "episode: 0/10, score: 5, e: 1.0, r: 5\n",
      "episode: 0/10, score: 6, e: 1.0, r: 5\n",
      "episode: 0/10, score: 7, e: 1.0, r: 4\n",
      "episode: 0/10, score: 8, e: 1.0, r: 2\n",
      "episode: 0/10, score: 9, e: 1.0, r: 0\n",
      "episode: 0/10, score: 10, e: 1.0, r: 2\n",
      "episode: 0/10, score: 11, e: 1.0, r: 4\n",
      "episode: 0/10, score: 12, e: 1.0, r: 6\n",
      "episode: 0/10, score: 13, e: 1.0, r: 7\n",
      "episode: 0/10, score: 14, e: 1.0, r: 6\n",
      "episode: 0/10, score: 15, e: 1.0, r: 7\n",
      "episode: 0/10, score: 16, e: 1.0, r: 5\n",
      "episode: 0/10, score: 17, e: 1.0, r: 4\n",
      "episode: 0/10, score: 18, e: 1.0, r: 6\n",
      "episode: 0/10, score: 19, e: 1.0, r: 5\n",
      "episode: 0/10, score: 20, e: 1.0, r: 5\n",
      "episode: 0/10, score: 21, e: 1.0, r: 6\n",
      "episode: 0/10, score: 22, e: 1.0, r: 6\n",
      "episode: 0/10, score: 23, e: 1.0, r: 7\n",
      "episode: 0/10, score: 24, e: 1.0, r: 6\n",
      "episode: 0/10, score: 25, e: 0.95, r: 10\n",
      "episode: 0/10, score: 26, e: 0.9, r: 11\n",
      "episode: 0/10, score: 27, e: 0.86, r: 12\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"SuperMarioBros-v2\")\n",
    "    env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
    "    state_size = env.observation_space.shape[1]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    \n",
    "    done = False\n",
    "    batch_size = 24\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        #state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "        # state = np.reshape(state, [1, state_size])\n",
    "        for time in range(250):\n",
    "            env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #next_state = cv2.cvtColor(next_state, cv2.COLOR_BGR2GRAY)\n",
    "            reward = reward if not done else -50\n",
    "            # next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.update_target_model()\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}, r: {}\".format(e, EPISODES, time, agent.epsilon, reward))\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
