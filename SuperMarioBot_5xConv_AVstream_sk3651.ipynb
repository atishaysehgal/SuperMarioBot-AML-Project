{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuperMarioBot: Using Deep Q-Learning to Play Super Mario Bros. (Columbia University MA Stats GR5242: Advanced Machine Learning Final Project Report)\n",
    "\n",
    "## Contributors\n",
    "\n",
    "* Sam Kolins (sk3651)\n",
    "* Atishay Sehgal (as5453)\n",
    "* Arpita Shah (as5451)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In 2013, researchers at Google DeepMind published the now-infamous paper **[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf)**, in which for the first time, a neural network algorithm learned how to play a video game in a fairly organic sort of way by playing against itself and training on the images produced from those play sessions. Since then, in just five short years, a whole host of enthusiastic data scientists have trained neural networks on a wide variety of different games. Most of these games have been Atari games, but for our project, we wanted to try something a bit more complicated: ***[Super Mario Bros. for the Nintendo Entertainment System (NES)](https://en.wikipedia.org/wiki/Super_Mario_Bros.)***. We are attempting here to train a deep Q-learning algorithm to play *Super Mario Bros.*; in particular, this means we want to get Mario to the end of the level before time runs out and without dying.\n",
    "\n",
    "## How to Play *Super Mario Bros.*\n",
    "\n",
    "### Objective and Level Design\n",
    "\n",
    "The goal of the game, as mentioned previously, is to get to the end of the level. This is not as easy as it sounds; along the way, Mario must overcome many obstacles in the form of moving enemies that can hurt Mario and bottomless pits that will instantly kill him if he falls into them. Mario also only starts with three lives; if Mario still has lives remaining, he can restart from the beginning of the current level, but **if he runs out of lives, he gets a *Game Over*, and must restart from World 1-1** (although, there is a little-known trick whereby pressing **Start** and **A** to begin a new game will place Mario at the first level of whichever world he died on; so for example, if Mario gets a Game Over on World 4-3, he can restart on World 4-1 instead of World 1-1 using this trick). \n",
    "\n",
    "In particular, *Super Mario Bros.* is a side-scrolling platformer, meaning Mario will need to continue moving right in order to access the end of the level. The end of the level is represented by a flag; if Mario touches this flag, the level automatically ends, and a cutscene plays in which Mario enters a castle (if your level timer ends in 1, 3, or 6, a number of fireworks worth 500 points each equal to the last timer digit will fire off). There are eight worlds with four levels apiece, bringing the game's level count to 32; each level is denoted by World $X$-$Y$, where $X$ is the number of the world and $Y$ is the number of the level. There are five kinds of levels (each having their own musical theme in the original NES version of *Super Mario Bros.*):\n",
    "\n",
    "* **Overworld**: these levels are the typical \"default\" platforming levels that take place outside atop solid ground. They do not tend to vary much in their terrain. The first level of each world is always an Overworld level; sometimes, the second or even third level of each world is an Overworld level as well.\n",
    "* **Underground**: at the beginning of each of these levels, Mario travels down a *__Warp Pipe__* (generally used for transportation in the *Mario* games, but can also sometimes contain enemies) where he finds himself underground. These levels have a darker color theme and a ceiling of blocks above Mario at the top of the screen (and more blocks in general). Often times, the second level of each world will be an Underground level.\n",
    "* **Underwater**: Mario starts the stage, and spends the entire stage, underwater. The physics of the entire level have completely changed as now Mario moves by swimming upward instead of jumping and falls much slower as a result. While Mario is not at any risk of drowning, there are unique swimming enemies in these levels that can still serve as potent threats due to their strange movement patterns (such as [Bloopers](https://i.etsystatic.com/9680903/r/il/26312a/620419040/il_570xN.620419040_rwuo.jpg)). Much like with Underground levels, the second level of each world is often Underwater (if it isn't Underground).\n",
    "* **Athletic**: These levels typically have little to no true ground areas at all as Mario traverses on the tops of [large trees](https://www.mariowiki.com/images/d/dd/SMB1W1-3.png). They may also make use of moving platforms (see previous pic). This increased verticality and general increase in bottomless pits can make them treacherous to navigate. The third level in each world is almost always Athletic with the exception of World 8-3, the game's penultimate level, which takes place in the Overworld.\n",
    "* **Castle**: In the last level of each world (so World $X$-$4$), Mario enters one of Bowser's castles (Bowser being the main villain of *Super Mario Bros.* and much of the *Mario* series in general). Much like the Underground levels, these levels are generally enclosed by a ceiling because they canonically take place indoors. Generally speaking, the most treacherous of obstacles will appear in these levels, many of which having a fire theme; bottomless pits are generally replaced by [lava pits](https://www.mariowiki.com/images/3/32/SMB1W1-4.png) (which can spit out their own fireballs called [Podoboos](https://vignette.wikia.nocookie.net/mario/images/4/48/Podoboo_1.jpg/revision/latest?cb=20120504221020) with little to no warning) and [Fire-Bars](https://www.mariowiki.com/images/3/3c/SMBW5-4.png) which can [spin around](https://www.mariowiki.com/images/7/74/FireRodSMB.gif) and hit the player are numerous. Bowser, standing on a bridge overlooking a lava pit, is directly blocking the end of the level (represented by an axe instead of a flag) and will spit fire traveling in straight lines at the player when he appears on-screen. If Mario touches this axe, the bridge disappears, Bowser falls into the pit, and Mario clears the level and the world, shortly followed by an [infamous cutscene](https://www.youtube.com/watch?v=n4ucO4xe28c).\n",
    "\n",
    "After beating the NES version of the game, the player unlocks a **hard mode**. This makes many modifications to the game, like turning [Goombas](https://ctl.s6img.com/society6/img/vG1nc6QSu4HFMq-cU71ibAx76z4/w_700/prints/~artwork/s6-0020/a/7729279_15115587/~~/pixel-goomba-super-mario-bros-prints.jpg), the game's weakest and most standard enemies, into [Buzzy Beetles](http://farm1.static.flickr.com/59/194964545_e89f8ff3e7.jpg) which are immune to fireballs. All enemies move faster, power-ups are no longer available, all moving platformers are made smaller, and Fire-Bars are far more common. Levels in the base game that are similar to each other (e.g. World 1-3 and 5-3 are almost the same except 5-3 is harder) are now made identical, making a lot of easier levels harder.\n",
    "\n",
    "### Controls\n",
    "\n",
    "The [NES controller](https://pisces.bbystatic.com/image2/BestBuy_US/images/products/5579/5579396_sd.jpg;maxHeight=640;maxWidth=550) has eight buttons: the ***D-pad*** (containing the four cardinal directions **up**, **down**, **left**, and **right**), **A**, **B**, **Start**, and **Select**. The function of each button is generally intuitive, but depends on context:\n",
    "\n",
    "* **Left** and **right** will move Mario left and right. The other two D-pad directions, **up** and **down**, are rarely used, but do have uses; **down** is used to access Warp Pipes and crouch as ***Super Mario*** (more on that later) while **up** is generally only used for traveling up vines.\n",
    "* **A** is used to jump or swim upwards.\n",
    "* **B** is used to run, throw a ***fireball*** (more on this later), restart the game at the end, or select a world.\n",
    "* **Start** pauses the game and confirms the selected option on the title screen.\n",
    "* **Select** is only used on the title screen to select the number of players; *Super Mario Bros.* was a multiplayer game on the NES where a second player could play as Mario's brother, Luigi. Control would alternate between the two players whenever any of them died.\n",
    "\n",
    "### Items and Enemies\n",
    "\n",
    "As mentioned before, each level has enemies that can hurt Mario if he touches them. Typically, the only way Mario can beat enemies is by jumping on them (although there are exceptions, like [Spinies](https://c1.staticflickr.com/1/67/194964203_92e02e0e72_m.jpg), which have spikes on their back that will hurt Mario if he touches them). Mario typically cannot survive a hit; that is, if an enemy touches him once (not counting Mario jumping on them), he dies and loses a life. However, Mario can collect an item known as a **[Super Mushroom](http://icons.iconarchive.com/icons/ph03nyx/super-mario/256/Retro-Mushroom-Super-icon.png)** (technically called a \"Magic Mushroom\" in the days of *Super Mario Bros.* and has since changed names) that will make him grow to twice his size. This allows Mario to break blocks above him (except the [? Block](https://res.cloudinary.com/teepublic/image/private/s--xm7dVDxa--/t_Preview/b_rgb:0195c3,c_limit,f_jpg,h_630,q_90,w_630/v1481855557/production/designs/955552_1.jpg), from which Mario can collect [coins](https://i.kinja-img.com/gawker-media/image/upload/s--dpkSgaoo--/c_fit,f_auto,fl_progressive,q_80,w_320/18j2qh36rzq56jpg.jpg) and items), crouch, and more importantly, take a hit without dying (although doing so will transform him back to his regular size). Mario can also collect a **[Fire Flower](http://icons.iconarchive.com/icons/ph03nyx/super-mario/256/Retro-Flower-Fire-icon.png)** which, in addition to granting Mario the effects of the Super Mushroom, also gives him the part to throw fireballs with **B** that can kill enemies (even Spinies). In this state, Mario has a distinct look - **Fire Mario** - in which his shirt turns red and his hat and overalls turn white. A comparison between all three standard versions of Mario can be found [here](https://i.imgur.com/q7FzrbE.png). Lastly, Mario can very rarely find a [Starman](http://rs997.pbsrc.com/albums/af99/Super_Mario_11/SMB1StarMan.gif~c200) inside a ? Block that grants him invincibility to any enemy he touches and also changes the music to this [catchy theme](https://www.youtube.com/watch?v=Lw049q22-hY) during the duration of its effect.\n",
    "\n",
    "## OpenAI Gym and `gym-super-mario-bros`\n",
    "\n",
    "The version of the game we are using for this project is not exactly the same as the NES version of Mario, however, as we do not have access to an authentic NES console. Instead, we are **using an NES emulator to run the game**, modified to run in Python (2.7, 3.5, or 3.6) and with some extra modifications for convenience. The package we are using, called **[`gym-super-mario-bros`](https://pypi.org/project/gym-super-mario-bros/)**, was created by Christian Kauten (\"kautenja\" on GitHub) and is an OpenAI Gym environment using the [`nes-py` emulator](https://pypi.org/project/gym-super-mario-bros/) (also made by Kauten) that can run both the original *Super Mario Bros.* and *Super Mario Bros. 2: The Lost Levels*. We are only concerned with *Super Mario Bros.* for this project.\n",
    "\n",
    "There are many changes made to the game, like running at a far smaller resolution than the NES was capable of and the lack of music, but the most important changes are summarized here:\n",
    "\n",
    "- Nearly all __cutscenes__ have been __removed from the game__. This includes Mario's death animation, the cutscenes that display the current level name and Mario's life count prior to the beginning of each level, the level clear cutscene (where Mario enters the castle just beyond the flag), the world clear cutscene, and so on. This is because the game cannot register inputs during these scenes and so there is no reason to have the neural network attempt to train on them. If a cutscene can't be removed by hacking the NES's RAM (as the environment attempts to do from `nes-py`), the program will lock any Python process attempting to train on the cutscene frames until the game is ready to accept inputs again.\n",
    "- __Levels can be loaded individually.__ Rather than play from the very beginning of the first level, we can instead decide to train a network on any particular level of our choosing. This can be a good way to train Mario on trickier levels that may have more difficult terrain (like Athletic or Castle levels) or enemies that only appear in certain levels. Doing so however means that a single episode contains only one life rather than three lives.\n",
    "- __There are multiple downsampled versions of the game.__ Downsampling is important because the reduction in rendering detail will make it easier for the convolutional layers of the deep Q-learning network to process the game images. There are three downsampled versions, in addition to `v0`, the original version of the game (see the `gym-super-mario-bros` documentation for more details):\n",
    "    * `v1` does not affect any foreground elements, but removes color from the background and simplifies the designs of background elements somewhat.\n",
    "    * `v2` further simplifies all in-game assets, including Mario and the in-game heads-up display (HUD), into blockier designs.\n",
    "    * `v3` takes this even further, simplifying all elements into colored rectangles (including Mario himself!).\n",
    "- The base game contains **4 frames of frameskip**. This means that, though each frame is calculated, only every fourth frame is drawn when the game is rendered. **This frameskip can be removed**, but it is not recommended for CPU's/GPU's that are not fast enough to render the game quickly.\n",
    "\n",
    "There are many more details about how the game tracks various statistics that we would like to optimize over (like Mario's x-position) or against (like the in-game clock). Please read the `gym-super-mario-bros` documentation for more details.\n",
    "\n",
    "## Final Goal\n",
    "\n",
    "There are many possible goals we can set for ourselves here. A very ambitious goal would be to train Mario to beat the entire game in a single episode... all 32 levels! We're going to be more realistic though, especially given our relative inexperience in Python and lack of time relative to the length of training times and say that a good goal would simply be to **beat World 1-1**. If not, we'd like to at least see how far we can get and offer possible improvements or reasons as to why we were unable to reach this goal.\n",
    "\n",
    "## Additional Notes\n",
    "\n",
    "This entire notebook was created *locally* in Python 3.6.6 because ***Colab has issues running `gym-super-mario-bros`*** (at least, for us; trying to load the Mario environment was giving us weird errors we were unable to diagnose). There it is ***strongly* recommended that you *do not attempt to run this in Google Colab***. Also, make sure that **Microsoft Visual C++ 14.0 is installed**, which can be acquired [here](https://www.scivision.co/python-windows-visual-c++-14-required/).\n",
    "\n",
    "And now, without further ado, let the learning begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Implementation: SuperMarioBot (5xConv AV-Stream Network)\n",
    "\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: this entire notebook was created LOCALLY in Python 3.6.6 because Colab has issues running gym-super-mario-bros\n",
    "\n",
    "# for me:\n",
    "# to access, open Anaconda Prompt and type \"activate py36\" followed by \"jupyter notebook\"\n",
    "# it might only need to be done once\n",
    "# does not need to be done on the desktop because I made sure to install Anaconda3 5.2.0 which has Python 3.6 by default\n",
    "\n",
    "# in console: \"python -m pip install --upgrade pip\" after \"activate py36\" to enter the Python 3.6 notebook\n",
    "# start by installing the package\n",
    "!pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the starter code, I had some missing DLL errors from trying to import `cv2` (which is a dependency used by either `nes_py` or `gym_super_mario_bros`), so installing the following package might help if you get the same issues. If not, I would recommend installing [Dependency Walker](http://dependencywalker.com/), which is free, which I ran on the `.pyd` file for my version of `cv2` to determine which dependencies I was missing and thereby manually downloaded them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-contrib-python\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/f3/5bbdaa05dfea18d32d5af00b6d25bed8d81a0c2eaa278a41052be689b1ba/opencv_contrib_python-3.4.4.19-cp36-cp36m-win_amd64.whl (44.1MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from opencv-contrib-python) (1.14.3)\n",
      "Installing collected packages: opencv-contrib-python\n",
      "Successfully installed opencv-contrib-python-3.4.4.19\n"
     ]
    }
   ],
   "source": [
    "# I had some missing DLL errors from trying to import cv2, so installing this might help if you have the same issues\n",
    "# if not, you might have to look for the missing DLL's manually. there's a free program called Dependency Walker that can help\n",
    "# locate missing DLL's\n",
    "!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Tensorboard in c:\\users\\sam\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from Tensorboard) (1.14.3)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from Tensorboard) (1.17.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from Tensorboard) (0.14.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\sam\\anaconda3\\lib\\site-packages (from Tensorboard) (0.31.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from Tensorboard) (3.0.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from Tensorboard) (1.11.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from Tensorboard) (3.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sam\\anaconda3\\lib\\site-packages (from protobuf>=3.4.0->Tensorboard) (39.1.0)\n"
     ]
    }
   ],
   "source": [
    "# install Tensorboard if needed for visualization\n",
    "!pip install Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned earlier that the NES controller has eight buttons. Because any combination of inputs can be registered on a single frame, that means there are $2^8 = 256$ possible NES controller inputs possible on a given frame. But *Super Mario Bros.* fails to make use of most of them; as discussed earlier, the **Select** and **Start** buttons (and even **up** and **down**) are highly niche buttons within the game, and because this emulation lacks a title screen (as there is no reason to train on it), we never need to press **Start** or **Select**. This seems like it would an extremely wasteful use of this action space, but luckily, `gym_super_mario_bros` contains three action libraries. One of these is `SIMPLE_MOVEMENT`, which contains the following seven inputs: **idle**, **right**, **right+A**, **right+B**, **right+A+B**, **A**, and **left**. `COMPLEX_MOVEMENT` adds button combinations that involve the direction **left** while `RIGHT_ONLY` removes any non-idle action that does not move Mario to the right.\n",
    "\n",
    "Why might Mario ever want to move left? He may want to dodge enemies or go back to get missed coins or power-ups, but there is a more pertinent use case here. Mario may be forced to enter a pipe that opens sideways, in which case if it is possible to jump on that pipe and keep moving forward past the opening like it is at [the end of World 1-2](https://www.mariowiki.com/images/f/fb/World_1-2_SMB.png), Mario is very likely to get stuck running into the wall unless he learns to move left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages needed for the rest of the project, except for gym_super_mario_bros stuff (in the next code block)\n",
    "# if you get a FutureWarning message about \"np.floating\" being deprecated or something, ignore it\n",
    "\n",
    "from __future__ import division\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input, Lambda\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# run this to allow Keras to save model weights\n",
    "# it should just run automatically because h5py is a dependency of Keras by default\n",
    "import h5py\n",
    "import os # also need this to create the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block if you just want to import without seeing the demo of Mario doing random stuff for 5000 steps\n",
    "# if you haven't seen that yet, you should!\n",
    "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing an environment in which Mario chooses a random action from the SIMPLE_MOVEMENT action library for 5000 steps\n",
    "# on the default Mario ROM's, there is a frameskip factor of 4 frames, so one step is really more like 4 frames\n",
    "\n",
    "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v2')\n",
    "env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "done = True\n",
    "for step in range(5000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network\n",
    "\n",
    "Now, we assemble the network architecture as an object instance of the class `CNN_DQNAgent`. The first half is a collection of five convolutional network layers; the second half is, for lack of a better term, going to be called the ***AV-stream***. This splits the output of the final convolutional layer into a *value stream* and an *advantage stream*. The former represents how well off Mario is in his current state; the latter represents how much better off Mario can be by taking a particular action. Advantage is essentially the difference between the Q function and the value stream. To get the estimated Q output, we must add back the advantage and value streams together; the highest advantage would then, of course, yield the highest Q output and therefore represent our optimal action for the current state.\n",
    "\n",
    "The basis of this network design was drawn from [Branko Blagojevic's great tutorial](https://medium.com/ml-everything/learning-from-pixels-and-deep-q-networks-with-keras-20c5f3a78a0) on reinforcement learning with CNN's and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can start to actually implement the deep Q-network (DQN) now\n",
    "\n",
    "class CNN_DQNAgent():\n",
    "    def __init__(self, input_shape, num_actions, final_layer_size = 128):\n",
    "        # The input image of Super Mario Bros. is 256x240x3 (RGB color channels)\n",
    "        # but env.observation_space.shape = (240, 256, 3), so for whatever reason it's flipping the images\n",
    "        # either that or I'm somehow wrong about the dimension conventions for gym_super_mario_bros/nes_py\n",
    "        # we will have to account for this in our model\n",
    "        self.inputs = Input(shape = input_shape, name = \"main_input\")\n",
    "        \n",
    "        # 5 convolutional layers (with relu activation) on the input\n",
    "        # Conv1: 32 filters, 24x24 with 4x4 striding\n",
    "        self.model = Conv2D(filters = 32, kernel_size = [24, 24], strides = [4, 4], \n",
    "                            activation = \"relu\", padding = \"valid\", name = \"conv1\")(self.inputs)\n",
    "        # Conv2: 64 filters, 13x17 with 2x2 striding\n",
    "        # the rectangular filters here are designed to make the output square (not counting depth of course)\n",
    "        self.model = Conv2D(filters = 64, kernel_size = [13, 17], strides = [2, 2], \n",
    "                            activation = \"relu\", padding = \"valid\", name = \"conv2\")(self.model)\n",
    "        # Conv3: 64 filters, 12x12 no striding (1x1)\n",
    "        self.model = Conv2D(filters = 64, kernel_size = [12, 12], strides = [1, 1], \n",
    "                            activation = \"relu\", padding = \"valid\", name = \"conv3\")(self.model)\n",
    "        # Conv4: 128 filters, 8x8 no striding\n",
    "        self.model = Conv2D(filters = 128, kernel_size = [8, 8], strides = [1, 1], \n",
    "                            activation = \"relu\", padding = \"valid\", name = \"conv4\")(self.model)\n",
    "        # Conv5: 128 (final_layer_size) filters, 4x4 no striding\n",
    "        self.model = Conv2D(filters = final_layer_size, kernel_size = [4, 4], strides = [1, 1], \n",
    "                            activation = \"relu\", padding = \"valid\", name = \"conv5\")(self.model)\n",
    "        # output size is now 1x1x128 with 1,884,576 trainable parameters\n",
    "        \n",
    "        # the final conv layer is separated into value and advantage streams\n",
    "        # value stream: current value in a given state\n",
    "        # advantage stream: change in value after making a particular move\n",
    "        # Advantage(state, action) = Q(state, action) - Value(state)\n",
    "        self.stream_AC = Lambda(lambda layer: layer[:, :, :, :final_layer_size // 2], \n",
    "                                name = \"adv_stream\")(self.model)\n",
    "        self.stream_VC = Lambda(lambda layer: layer[:, :, :, final_layer_size // 2:], \n",
    "                                name = \"val_stream\")(self.model)\n",
    "        \n",
    "        # flatten the advantage and value functions\n",
    "        self.stream_AC = Flatten(name = \"adv_flat\")(self.stream_AC)\n",
    "        self.stream_VC = Flatten(name = \"val_flat\")(self.stream_VC)\n",
    "        \n",
    "        # define weights for advantage and value layers\n",
    "        # these will be trained so that the matmul matches the expected adv/val from play\n",
    "        self.Advantage = Dense(num_actions, name = \"final_advantage\")(self.stream_AC)\n",
    "        self.Value = Dense(1, name = \"final_value\")(self.stream_VC)\n",
    "        \n",
    "        # add value and advantage to get Q-output\n",
    "        # advantage is evaluated based on how good the action is relative to average state adv\n",
    "        self.model = Lambda(lambda val_adv: val_adv[0] + (val_adv[1] - K.mean(val_adv[1], axis = 1, keepdims = True)),\n",
    "                           name = \"final_out\")([self.Value, self.Advantage])\n",
    "        self.model = Model(self.inputs, self.model)\n",
    "        self.model.compile(\"adam\", \"mse\")\n",
    "        self.model.optimizer.lr = 0.0001 # learning rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a snippet from an Excel document in which I have listed out each layer of the network and calculated the number of parameters it adds to the total number of trainable parameters. In the end, we have $\\approx 1.88\\mathrm{M}$ trainable parameters.\n",
    "\n",
    "![x](https://i.imgur.com/SFV5rET.png)\n",
    "\n",
    "We can now proceed to create the agent instances that contain our models (this does not need to happen with explicit variable initialization or any mention of a `tf.Session()` object at all because we are using Keras and not TensorFlow). The main network will be saved to `main_qn` while the target network will be saved to `target_qn`. The significance of the target network will be explained later. For now, we just worry about creating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 240, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 55, 59, 32)   55328       main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 22, 22, 64)   452672      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 11, 11, 64)   589888      conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 4, 4, 128)    524416      conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv2D)                  (None, 1, 1, 128)    262272      conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "val_stream (Lambda)             (None, 1, 1, 64)     0           conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "adv_stream (Lambda)             (None, 1, 1, 64)     0           conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "val_flat (Flatten)              (None, 64)           0           val_stream[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "adv_flat (Flatten)              (None, 64)           0           adv_stream[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "final_value (Dense)             (None, 1)            65          val_flat[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "final_advantage (Dense)         (None, 7)            455         adv_flat[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "final_out (Lambda)              (None, 7)            0           final_value[0][0]                \n",
      "                                                                 final_advantage[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,885,096\n",
      "Trainable params: 1,885,096\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Q-network setup\n",
    "main_qn = CNN_DQNAgent(input_shape = (240, 256, 3), num_actions = len(SIMPLE_MOVEMENT), final_layer_size = 128)\n",
    "target_qn = CNN_DQNAgent(input_shape = (240, 256, 3), num_actions = len(SIMPLE_MOVEMENT), final_layer_size = 128)\n",
    "# len(SIMPLE_MOVEMENT) = 7\n",
    "\n",
    "# running a summary call to confirm my parameter calculations are correct\n",
    "main_qn.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer, Parameters, Helper Function\n",
    "\n",
    "Now, we wish to implement an ***experience replay system*** that stores game experiences. This too is represented as an instance of a class, this time called `ExperienceReplay`. To train from the network, we sample a batch of these from the trove of game experiences we have collected. We also cycle out older experiences for newer, more relevant ones past a certain limit defined here by `buffer_size`. Again, Blagojevic's tutorial was very helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        \"\"\" Data structure used to hold experiences \"\"\"\n",
    "        # each element of buffer contains a list: [state, action, reward, next_state, done]\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        \"\"\" Adds list of experiences to the buffer \"\"\"\n",
    "        # extend stored experiences\n",
    "        self.buffer.extend(experience)\n",
    "        # keep last buffer_size number of experiences\n",
    "        self.buffer = self.buffer[-self.buffer_size:]\n",
    "        # this construction better than using a control statement on current length of buffer\n",
    "        \n",
    "    def sample(self, size):\n",
    "        \"\"\" Returns a sample of experiences from the buffer \"\"\"\n",
    "        sample_idxs = np.random.randint(len(self.buffer), size = size)\n",
    "        sample_output = [self.buffer[idx] for idx in sample_idxs]\n",
    "        sample_output = np.reshape(sample_output, (size, -1))\n",
    "        return sample_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, we occasionally pull from `ExperienceReplay` to train the network. We then continue playing using the updated network after the weights have been changed. The **target network** is used to determine what the values should be based on the current state and action. In the next section, we will actually implement how training the main model is supposed to work, but first, we must initialize a host of useful parameters that will be used for training. Each line is commented with a brief description of what each variable represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing useful parameters\n",
    "batch_size = 64 # number of experiences used for each training step\n",
    "num_epochs = 20 # number of epochs to train\n",
    "update_freq = 3 # frequency of network updates\n",
    "y = 0.99 # discount factor\n",
    "prob_random_start = 0.6 # initial epsilon (chance of random action in epsilon-greedy policy)\n",
    "prob_random_end = 0.1 # final epsilon\n",
    "epsilon_steps = 27 # number of times epsilon decays from init -> final (should be = # of training episodes)\n",
    "num_episodes = 30 # number of episodes to train; remember that one episode = 3 lives!\n",
    "pre_train_episodes = 3 # number of episodes of random action (to build replay buffer)\n",
    "max_num_step = 50000 # maximum episode length in units of steps\n",
    "load_model = True # if True, will load a saved model\n",
    "path = \"./models\" # path for saved model\n",
    "main_weights_file = path + \"/main_weights.h5\" # file that will contain saved main weights\n",
    "target_weights_file = path + \"/target_weights.h5\" # same, but for target weights\n",
    "final_layer_size = 128 # size of final conv layer (before AV-stream split)\n",
    "tau = 1 # rate to update target network toward primary network\n",
    "\n",
    "# note: num_episodes - pre_train_episodes is number of training episodes\n",
    "\n",
    "# if we wish to stop training after surpassing a certain mean reward, specify that goal here\n",
    "# I'm leaving this commented out (and the print segment for surpassing the goal as well)\n",
    "# because I don't yet have a sense of how much reward corresponds to, say, beating a level yet\n",
    "# presumably, we could also read from the info dict returned by each step to detect for changes in level tuple\n",
    "# goal = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful notes:\n",
    "\n",
    "* Remember that a single episode consists of three lives. Since we aren't accounting for changes in behavior based on the number of lives as that is not a part of the reward function (to be discussed later), our strategy is independent of the current life count, and so one-life episodes would be indistinguishable from three-life episodes. We haven't adjusted the *Super Mario Bros.* package to reflect this difference (although we could choose to load a single level if we wanted one-life episodes), but it does mean we can run on fewer episodes and still get good results. Choosing $333$ episodes, for example, as I have done here by default, still gets us $333 \\times 3 = 999$ lives worth of training.\n",
    "* This algorithm uses an $\\epsilon$-greedy policy, but introduces an interesting variation in which $\\epsilon$ will vary over time. Mario will therefore be fairly experimental in the earliest stages of the stage, trying a host of different options to collect as many game states as possible (to say nothing of the pre-training episodes in which Mario will only act randomly), but will be increasingly looking to optimize his reward over time.\n",
    "* We want `max_num_step` to be very large. As you might have seen running a completely random sample of Mario actions a few code blocks ago, $5000$ steps is enough for an episode or two, but that's about it. Once Mario gets better at running through levels, each episode could be very long; we don't want to cut off too many potentially successful level completion runs early.\n",
    "* The inclusion of `final_layer_size` is technically unnecessary as we have given it the default value of `128` when we were defining our network agent. Because it is important in determining the sizes of our advantage and value streams though, it feels like something that should technically be modifiable via changing a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more thing we need before we can start the actual training: a helper function called `update_target_graph`. This function updates the target graph `target_graph` with values from the main graph `main_graph` via a weighted average of the values in each graph. The weight for this average is determined by `tau`. For an example of how this might work, suppose `main_graph = [0, 1, 2]`, `target_graph = [3, 4, 5]`, and $\\tau = 0.1$. Then the values updated to the target graph, written in matrix multiplication form, are\n",
    "\n",
    "$$ \\left[\\begin{matrix}\n",
    "0 & 3 \\\\\n",
    "1 & 4 \\\\\n",
    "2 & 5 \\end{matrix}\\right] \\cdot \\left[\\begin{matrix}\n",
    "\\tau \\\\\n",
    "1 - \\tau \\end{matrix}\\right] = \\left[\\begin{matrix}\n",
    "0 & 3 \\\\\n",
    "1 & 4 \\\\\n",
    "2 & 5 \\end{matrix}\\right] \\cdot \\left[\\begin{matrix}\n",
    "0.1 \\\\\n",
    "0.9 \\end{matrix}\\right] = \\left[\\begin{matrix}\n",
    "2.7 \\\\\n",
    "3.7 \\\\\n",
    "4.7 \\end{matrix}\\right]. $$\n",
    "\n",
    "The function is not actually doing this as a matrix multiplication though, rather it is simply taking a weighted average for each entry, as in `[(0 * 0.1) + (3 * 0.9), (1 * 0.1) + (4 * 0.9), (2 * 0.1) + (5 * 0.9)] = [2.7, 3.7, 4.7]`. Note that we do not need to store these weights in any sort of variable; a simple call to `set_weights()` will calibrate them for us. Because we are not using TensorFlow, we do not need to save these into a tensor object and then make sure the session runs on this tensor object in addition to the rest of the graph. This is a nice convenience that the use of Keras grants to us.\n",
    "\n",
    "The code for this won't be found on Blagojevic's tutorial. Instead, it can be found [on his GitHub here](https://github.com/breeko/Simple-Reinforcement-Learning-with-Tensorflow/blob/master/Part%204%20-%20Deep%20Q-Networks%20and%20Beyond%20with%20Keras.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for updating target graph with values from main graph\n",
    "def update_target_graph(main_graph, target_graph, tau):\n",
    "    updated_weights = (np.array(main_graph.get_weights()) * tau) + (np.array(target_graph.get_weights()) * (1 - tau))\n",
    "    target_graph.set_weights(updated_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can actually start training. The basis for this code can also be found on Blagojevic's GitHub (see above), but because we're training on *Super Mario Bros.* and not Blagojevic's peculiar custom game (which, funnily enough, is from a package called `gridworld`, even though it's not the GridWorld game from class), we needed to make a number of critical changes. For example, the tutorial makes no use of Tensorboard whatsoever, so it took some effort to figure out to display the statistics I wanted to display using Keras. I also needed to learn how to access variables and functions specific to `gym_super_mario_bros`, such as the reward function, because it's not as simple as it is in Blagojevic's custom `gridworld` game. In fact, perhaps we will take a minute to briefly explain it...\n",
    "\n",
    "### The Reward Function\n",
    "\n",
    "The documentation for `gym_super_mario_bros` defines the reward per step $r$ as\n",
    "\n",
    "$$ r = v + c + d $$\n",
    "\n",
    "where:\n",
    "\n",
    "* $v$ is Mario's **instantaneous velocity**. More precisely, it is the difference in the agent's $x$-position between states. Therefore, if $x_0$ is Mario's initial position and $x_1$ is Mario's position after advancing a step, then $v = x_1 - x_0$. As a consequence, $v > 0$ implies Mario is moving right, $v < 0$ implies Mario is moving left, and $v = 0$ implies Mario has not moved at all (at least not horizontally; perhaps he is jumping/falling in place or climbing a vine).\n",
    "* $c$ is the **difference in game clock between steps**. More precisely, if $c_0$ and $c_1$ are the in-game times before and after the step respectively, then $c = c_0 - c_1$. Because the timer decreases as the game continues, $c$ is never a positive number; either $c = 0$ and the timer hasn't decreased at all (possible because one in-game \"second\" is roughly equivalent to $0.4$ real-world seconds, a length of time still longer than a frame or even $4$ frames in the frameskip version of the NES ROM we are using) or $c < 0$ and the in-game clock has ticked down. This is essentially insignificant to Mario's reward at best and a penalty at worst, which is by design as we don't want Mario to spend too much time standing still.\n",
    "* Finally, $d$ is a **death penalty** that penalizes the agent for dying in a state. This is a hefty penalty that strongly discourages the agent from dying, which will help motivate it to learn what causes Mario to die in the first place. If Mario is alive, $d = 0$, but if Mario dies, $d = -15$. Because the game has been scrubbed of (virtually) all cutscenes, the death penalty should only penalize Mario for a single step.\n",
    "\n",
    "Interestingly, the network is not intended to grab these values from trying to interpret the pixels of each step/frame; instead, `gym_super_mario_bros` contains functions that return and compute these values directly from `nes_py` memory. This makes computation of the reward for each state relatively easy.\n",
    "\n",
    "After computing $r$, there is one final adjustment made to the reward: it is clipped into the interval $[-15, 15]$. That means Mario cannot gain or lose more than $15$ points on a single step. Because this value is also equal to the lowest possible value of $d$ (acquired upon death), Mario cannot do any worse on a single step than dying, which establishes death as the game's ultimate single-state penalty. Of course, in the long term, this likely won't greatly hinder Mario's reward score as Mario can only die three times in a single episode, totaling $-45$ points, but it works as an excellent short-term motivator against interacting with anything that might kill Mario and should still force the agent to think more intelligently about traversing each course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "\n",
    "*(Note: if you render the environment and it freezes every once in a while, that's just the deep Q-network going through the training motions. You'll know this because it will freeze periodically in precise accordance with the number of episodes it takes before the model decides to update again, saved to* `update_freq`*.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (re)initializing the environment (we closed it before with env.close())\n",
    "# or if you haven't seen the demo, this would be the first time you're running it\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v2')\n",
    "env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/30, Mean reward: 1797.0000, Epsilon: 0.6000, Loss: 0.0000\n",
      "Episode: 2/30, Mean reward: 1340.0000, Epsilon: 0.6000, Loss: 0.0000\n",
      "Episode: 3/30, Mean reward: 1769.0000, Epsilon: 0.6000, Loss: 0.0000\n",
      "Episode: 4/30, Mean reward: 630.0000, Epsilon: 0.6000, Loss: 0.0000\n",
      "Episode: 5/30, Mean reward: 1828.0000, Epsilon: 0.5955, Loss: 0.0000\n",
      "Episode: 6/30, Mean reward: 2285.0000, Epsilon: 0.5910, Loss: 110.7372\n",
      "Episode: 7/30, Mean reward: 1900.0000, Epsilon: 0.5865, Loss: 110.7372\n",
      "Episode: 8/30, Mean reward: 1565.0000, Epsilon: 0.5820, Loss: 110.7372\n",
      "Episode: 9/30, Mean reward: 1405.0000, Epsilon: 0.5775, Loss: 110.7372\n",
      "Episode: 10/30, Mean reward: 2397.0000, Epsilon: 0.5730, Loss: 110.7372\n",
      "Episode: 11/30, Mean reward: 2364.0000, Epsilon: 0.5685, Loss: 24.6795\n",
      "Episode: 12/30, Mean reward: 1624.0000, Epsilon: 0.5640, Loss: 24.6795\n",
      "Episode: 13/30, Mean reward: 1886.0000, Epsilon: 0.5595, Loss: 24.6795\n",
      "Episode: 14/30, Mean reward: 2172.0000, Epsilon: 0.5550, Loss: 24.6795\n",
      "Episode: 15/30, Mean reward: 2576.0000, Epsilon: 0.5505, Loss: 24.6795\n",
      "Episode: 16/30, Mean reward: 2025.0000, Epsilon: 0.5459, Loss: 25.9793\n",
      "Episode: 17/30, Mean reward: 2370.0000, Epsilon: 0.5414, Loss: 25.9793\n",
      "Episode: 18/30, Mean reward: 1199.0000, Epsilon: 0.5369, Loss: 25.9793\n",
      "Episode: 19/30, Mean reward: 2230.0000, Epsilon: 0.5324, Loss: 25.9793\n",
      "Episode: 20/30, Mean reward: 2368.0000, Epsilon: 0.5279, Loss: 25.9793\n",
      "Episode: 21/30, Mean reward: 2028.0000, Epsilon: 0.5234, Loss: 24.2772\n",
      "Episode: 22/30, Mean reward: 1449.0000, Epsilon: 0.5189, Loss: 24.2772\n",
      "Episode: 23/30, Mean reward: 1386.0000, Epsilon: 0.5144, Loss: 24.2772\n",
      "Episode: 24/30, Mean reward: 1181.0000, Epsilon: 0.5099, Loss: 24.2772\n",
      "Episode: 25/30, Mean reward: 1920.0000, Epsilon: 0.5054, Loss: 24.2772\n",
      "Episode: 26/30, Mean reward: 1476.0000, Epsilon: 0.5009, Loss: 26.4692\n",
      "Episode: 27/30, Mean reward: 1360.0000, Epsilon: 0.4964, Loss: 26.4692\n",
      "Episode: 28/30, Mean reward: 1323.0000, Epsilon: 0.4919, Loss: 26.4692\n",
      "Episode: 29/30, Mean reward: 2066.0000, Epsilon: 0.4874, Loss: 26.4692\n",
      "Episode: 30/30, Mean reward: 1297.0000, Epsilon: 0.4829, Loss: 26.4692\n"
     ]
    }
   ],
   "source": [
    "# Reset everything\n",
    "K.clear_session()\n",
    "\n",
    "# Q-network setup\n",
    "main_qn = CNN_DQNAgent(input_shape = env.observation_space.shape, \n",
    "                       num_actions = len(SIMPLE_MOVEMENT), \n",
    "                       final_layer_size = final_layer_size)\n",
    "target_qn = CNN_DQNAgent(input_shape = env.observation_space.shape, \n",
    "                         num_actions = len(SIMPLE_MOVEMENT), \n",
    "                         final_layer_size = final_layer_size)\n",
    "# len(SIMPLE_MOVEMENT) = 7\n",
    "\n",
    "# Make the networks equal\n",
    "update_target_graph(main_qn.model, target_qn.model, 1)\n",
    "\n",
    "# experience replay setup; where episodes are stored and grabbed for learning\n",
    "experience_replay = ExperienceReplay()\n",
    "\n",
    "# implementing epsilon variable with decay\n",
    "# in the beginning, Mario will act almost completely randomly (completely random = pre-training)\n",
    "# but over time, we will reduce the chance of random action in exchange for maximizing reward via Q function\n",
    "prob_random = prob_random_start\n",
    "prob_random_drop = (prob_random_start - prob_random_end) / epsilon_steps\n",
    "\n",
    "# more variables real quick\n",
    "num_steps = [] # tracks step count per episode\n",
    "rewards = [] # tracks rewards per episode\n",
    "total_steps = 0 # tracks cumulative steps taken during training\n",
    "print_every = 1 # how often episode progress gets printed\n",
    "save_every = 1 # how often we save\n",
    "losses = [0] # tracking training losses\n",
    "\n",
    "episodeID = 0 # episode number\n",
    "\n",
    "# path setup for saving\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# load weights if they are present\n",
    "if load_model == True:\n",
    "    if os.path.exists(main_weights_file):\n",
    "        print(\"Loading main weights...\")\n",
    "        main_qn.model.load_weights(main_weights_file)\n",
    "    if os.path.exists(target_weights_file):\n",
    "        print(\"Loading target weights...\")\n",
    "        main_qn.model.load_weights(target_weights_file)\n",
    "        \n",
    "while episodeID < num_episodes:\n",
    "    \n",
    "    \"\"\" Get ready to run another episode! \"\"\"\n",
    "    \n",
    "    # create experience replay for current episode\n",
    "    episode_buffer = ExperienceReplay()\n",
    "    \n",
    "    # get game state from the environment\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False # is game complete?\n",
    "    sum_rewards = 0 # running sum of rewards in episode\n",
    "    cur_step = 0 # running number of steps in episode\n",
    "    \n",
    "    while cur_step < max_num_step and not done:\n",
    "        # render the environment (this is entirely optional)\n",
    "        # commenting out may lead to faster train times but you won't be able to visualize progress\n",
    "        env.render()\n",
    "        \n",
    "        # increment step counter\n",
    "        cur_step += 1\n",
    "        total_steps += 1\n",
    "        \n",
    "        if np.random.rand() < prob_random or episodeID < pre_train_episodes:\n",
    "            # act randomly if Unif[0,1] < epsilon or if we are still in the pre-training phase\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Q network decides what action to take\n",
    "            action = np.argmax(main_qn.model.predict(np.array([state])))\n",
    "            \n",
    "        # take action and advance the step, receiving next state/reward/done\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # episode setup for storage in replay buffer\n",
    "        episode = np.array([[state], action, reward, [next_state], done])\n",
    "        episode = episode.reshape(1, -1)\n",
    "        \n",
    "        # store the experience in the episode buffer\n",
    "        episode_buffer.add(episode)\n",
    "        \n",
    "        # update cumulative rewards total\n",
    "        sum_rewards += reward\n",
    "        \n",
    "        # update the state\n",
    "        state = next_state\n",
    "        \n",
    "    \"\"\" After enough episodes, we train the network... \"\"\"\n",
    "        \n",
    "    if episodeID > pre_train_episodes:\n",
    "        if prob_random > prob_random_end:\n",
    "            # decrease probability of random action\n",
    "            prob_random -= prob_random_drop\n",
    "            \n",
    "        if episodeID % update_freq == 0:\n",
    "            for epoch in range(num_epochs):\n",
    "                # remember: train batch is a list of [S, A, R, S', done] lists...\n",
    "                train_batch = experience_replay.sample(batch_size)\n",
    "                \n",
    "                # separate batch into components\n",
    "                train_state, train_action, train_reward, train_next_state, train_done = train_batch.T\n",
    "                \n",
    "                # convert action array to int array for indexing\n",
    "                train_action = train_action.astype(np.int)\n",
    "                \n",
    "                # stack states for learning\n",
    "                train_state = np.vstack(train_state)\n",
    "                train_next_state = np.vstack(train_next_state)\n",
    "                \n",
    "                # predicted action from main Q network\n",
    "                target_q = target_qn.model.predict(train_state)\n",
    "                \n",
    "                # Q values from our target network for the next state\n",
    "                target_q_next_state = main_qn.model.predict(train_next_state)\n",
    "                train_next_state_action = np.argmax(target_q_next_state, axis = 1)\n",
    "                train_next_state_action = train_next_state_action.astype(np.int)\n",
    "                \n",
    "                # if we have a Game Over, we don't want to train on that step\n",
    "                # rewards will be multiplied by this value to ensure they aren't trained on\n",
    "                train_gameover = train_done == 0\n",
    "                \n",
    "                # Q value of next state based on predicted action\n",
    "                train_next_state_values = target_q_next_state[range(batch_size), train_next_state_action]\n",
    "                \n",
    "                # reward from action chosen in the training batch\n",
    "                true_reward = train_reward + (y * train_next_state_values * train_gameover)\n",
    "                target_q[range(batch_size), train_action] = true_reward\n",
    "                \n",
    "                # train the main model\n",
    "                loss = main_qn.model.train_on_batch(train_state, target_q)\n",
    "                losses.append(loss)\n",
    "                \n",
    "            # use main model to update target model\n",
    "            update_target_graph(main_qn.model, target_qn.model, tau)\n",
    "            \n",
    "            if (episodeID + 1) % save_every == 0:\n",
    "                # save the model\n",
    "                main_qn.model.save_weights(main_weights_file)\n",
    "                target_qn.model.save_weights(target_weights_file)\n",
    "                \n",
    "    # increment the episode (current episode is now finished)\n",
    "    episodeID += 1\n",
    "    \n",
    "    # dump episodic replay buffer into global replay buffer\n",
    "    # also, record number of steps and reward amount for that episode\n",
    "    experience_replay.add(episode_buffer.buffer)\n",
    "    num_steps.append(cur_step)\n",
    "    rewards.append(sum_rewards)\n",
    "    \n",
    "    \"\"\" Lastly, we need an indication of current progress. So, we print after every 50 (print_every) episodes... \"\"\"\n",
    "    \n",
    "    if episodeID % print_every == 0:\n",
    "        mean_loss = np.mean(losses[-(print_every * num_epochs):])\n",
    "        \n",
    "        print(\"Episode: {}/{}, Mean reward: {:0.4f}, Epsilon: {:0.4f}, Loss: {:0.4f}\". format(\n",
    "            episodeID, num_episodes, np.mean(rewards[-print_every:]), prob_random, mean_loss))\n",
    "        # if np.mean(rewards[-print_every:]) >= goal:\n",
    "        #    print(\"Training complete!\")\n",
    "        #    break\n",
    "\n",
    "# close the environment (make sure to reinitialize it with the previous code block if you want to train on loaded weights!)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One final save for the weights once we're done\n",
    "main_qn.model.save_weights(main_weights_file)\n",
    "target_qn.model.save_weights(target_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many steps did we take in this run? Run this to find out!\n",
    "total_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my initial run of the code, I used the following parameter settings (not including parameters that are not really intended for changing, like path location variables, `load_model`, `final_layer_size`, or `tau`):\n",
    "\n",
    "```\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "update_freq = 3\n",
    "y = 0.99\n",
    "prob_random_start = 0.6\n",
    "prob_random_end = 0.1\n",
    "epsilon_steps = 27\n",
    "num_episodes = 30\n",
    "pre_train_episodes = 3\n",
    "max_num_step = 50000\n",
    "print_every = 1\n",
    "save_every = 1\n",
    "```\n",
    "\n",
    "This means we ran a total of $30$ episodes, on which $27$ were used for training. For this reason, a choice of `update_freq = 3` makes sense, as then we will have updated $\\frac{27}{3} = 9$ times total. Of course, there is a trade-off here; the more times we update, the more Mario will learn, but the slower our training process becomes. If we want $\\epsilon$ to reach its ending value, we need to make sure that `epsilon_steps` is equal to the number of training episodes ran, so `epsilon_steps` is recommended to be set to the `num_episodes - pre_train_episodes`, which in this case is `27`. On this point, I would not recommend setting the value of `pre_train_episodes` too high because at some point, pre-train episodes will pop their own steps out from the replay buffer, which basically means they're completely wasted as Mario will never use them to train. It's hard to say how long $50000$ steps really lasts in terms of episodes, but since we can roughly run an episode in $5000$ steps, I would probably not set `pre_train_episodes` much higher than about `10` or `12`.\n",
    "\n",
    "Here, we used `prob_random_start = 0.6`, which is a relatively aggressive starting point as it means Mario will be looking to optimize his reward about $40%$ of the time. One might prefer a higher starting value for $\\epsilon$ as in the beginning, there isn't much to optimize (and we will see that this aggression actually hurts his short-term reward relative to random actions), but this will also slow down Mario's training, so there's a trade-off here as well. I highly recommend not setting `prob_random_end = 0` as there is always more Mario can learn from the game by randomly exploring (but perhaps the final value is better set less than `0.1`; this is part of the empiricism of machine learning, after all!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results\n",
    "\n",
    "It's hard to get a sense of how well Mario is learning just by reading code output. Therefore, I've decided to supply a few plots to assist in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots of number of steps and number of cumulative steps per episode\n",
    "x = list(range(1, num_episodes + 1))\n",
    "\n",
    "fig = plt.figure(figsize = (12, 6))\n",
    "fig.subplots_adjust(hspace = 6)\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.plot(x, num_steps, color = \"r\", linewidth = 2)\n",
    "plt.xlabel(\"Episode ID\")\n",
    "plt.ylabel(\"Number of steps\")\n",
    "plt.title(\"Number of Steps Per Episode\")\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.fill_between(x, np.cumsum(num_steps))\n",
    "plt.xlabel(\"Episode ID\")\n",
    "plt.ylabel(\"Number of steps\")\n",
    "plt.title(\"Number of Cumulative Steps Per Episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling average\n",
    "s = pd.Series(rewards, index = range(1, 1 + num_episodes))\n",
    "print(s)\n",
    "roll = s.rolling(window = 3).mean()\n",
    "print(list(roll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots of rewards and rolling average of rewards (window = 3) per episode\n",
    "fig = plt.figure(figsize = (12, 6))\n",
    "fig.subplots_adjust(hspace = 6)\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.plot(x, rewards, color = \"b\", linewidth = 2)\n",
    "plt.xlabel(\"Episode ID\")\n",
    "plt.ylabel(\"Episode reward\")\n",
    "plt.title(\"Reward Amount Per Episode\")\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.plot(x, list(roll), color = \"m\", linewidth = 2)\n",
    "plt.xlabel(\"Episode ID\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "plt.title(\"Rolling Average of Episode Rewards (N = 3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of losses per episode\n",
    "fig = plt.figure(figsize = (6, 6))\n",
    "fig.add_subplot(1, 1, 1)\n",
    "plt.plot(x, losses, color = \"g\", linewidth = 2)\n",
    "plt.xlabel(\"Episode ID\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Per Episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
